{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFFF\">\n",
    "<img src=\"logo.jpg\" width=150 ALIGN=\"left\" style='margin-right:10px; border-style: solid; border-width: 2px;' alt='logo'>\n",
    "<h1>Starting Kit - Iris Classification </h1>\n",
    "<p>\n",
    "This starting kit will guide you step by step and will walk you through the data statistics and<br>\n",
    "examples. This will give you a clear idea of what this challenge is about and how you can<br>\n",
    "proceed further to solve the challenge.\n",
    "</p>\n",
    "\n",
    "<br><br>\n",
    "<hr style='background-color: #D3D3D3; height: 1px; border: 0;'>\n",
    "<p>\n",
    "This code was tested with Python 3.8.5 |Anaconda custom (64-bit)| (default, Dec 23 2020, 21:19:02) (https://anaconda.org/)<br>\n",
    "</p>\n",
    "<hr style='background-color: #D3D3D3; height: 1px; border: 0;'>\n",
    "    \n",
    "<p>\n",
    "ALL INFORMATION, SOFTWARE, DOCUMENTATION, AND DATA ARE PROVIDED \"AS-IS\". The CHALEARN, AND/OR OTHER ORGANIZERS OR CODE AUTHORS DISCLAIM ANY EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR ANY PARTICULAR PURPOSE, AND THE WARRANTY OF NON-INFRIGEMENT OF ANY THIRD PARTY'S INTELLECTUAL PROPERTY RIGHTS. IN NO EVENT SHALL AUTHORS AND ORGANIZERS BE LIABLE FOR ANY SPECIAL, \n",
    "INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF SOFTWARE, DOCUMENTS, MATERIALS, PUBLICATIONS, OR INFORMATION MADE AVAILABLE FOR THE CHALLENGE. \n",
    "</p>\n",
    "\n",
    "<hr style='background-color: #D3D3D3; height: 1px; border: 0;'>\n",
    "    <p>\n",
    "This is a sample tabular data challenge for <b>Creation of an AI Challenge class</b> at  <b><a style='color: #62023C;' href='https://www.universite-paris-saclay.fr/'>Université Paris Saclay</a></b>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "<hr style='background-color: #D3D3D3; height: 1px; border: 0;'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Introduction\n",
    "\n",
    "This challenge uses a **Iris Flower Dataset** from **Kaggle(https://www.kaggle.com/datasets/arshid/iris-flower-dataset)**\n",
    "    \n",
    "The dataset coniststs of some flower features:\n",
    "1. sepal length\n",
    "2. sepal width\n",
    "3. petal length\n",
    "4. petal width\n",
    "\n",
    "The `label` shows the specie of the flower. \n",
    "\n",
    "    \n",
    "This challenge is about creating and predicting a Machine Learning model and train it with the iris data  to classify the flowers into correct species.\n",
    "\n",
    "\n",
    "**References and credits:**  \n",
    " - Iris Flowers Dataset (https://www.kaggle.com/datasets/arshid/iris-flower-dataset)\n",
    " - Université Paris Saclay (https://www.universite-paris-saclay.fr/)  \n",
    " - ChaLearn (http://www.chalearn.org/)  \n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'sample_code_submission/' # Change the model to a better one once you have one!\n",
    "result_dir = 'sample_result_submission/' \n",
    "problem_dir = 'ingestion_program/'  \n",
    "score_dir = 'scoring_program/'\n",
    "from sys import path; path.append(model_dir); path.append(problem_dir); path.append(score_dir); \n",
    "%matplotlib inline\n",
    "# Uncomment the next lines to auto-reload libraries (this causes some problem with pickles in Python 3)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import seaborn as sns; sns.set()\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Step 1: Exploratory data analysis\n",
    "We provide `sample_data` with the starting kit, but to prepare your submission, you must fetch the `public_data` from the challenge website and point to it.\n",
    "\n",
    "The data used for this challenge has images resized into 128x128 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'iris_challenge' # DO NOT CHANGE\n",
    "data_dir = 'sample_data' # Change it to point to the directory with public_data\n",
    "# !ls $data_dir*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###-------------------------------------###\n",
      "### Checking Data\n",
      "###-------------------------------------###\n",
      "\n",
      "\n",
      "###-------------------------------------###\n",
      "### Data Loaded!\n",
      "###-------------------------------------###\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from data_io import read_data\n",
    "data_train, data_test = read_data(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data: 75\n",
      "Test Data: 75\n",
      "Features: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Data:\", data_train.shape[0])\n",
    "print(\"Test Data:\", data_test.shape[0])\n",
    "\n",
    "print(\"Features:\", data_train.shape[1]-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Iris-virginica     25\n",
       "Iris-setosa        25\n",
       "Iris-versicolor    25\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Step 2: Building a predictive model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a predictive model\n",
    "We provide an example of predictive model in the `sample_code_submission/` directory. \n",
    "You can change this model and use a better one to get a good score for the challenge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install pre-requisites for using the model from Keras**\n",
    "\n",
    "Uncomment the next line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --user --upgrade tensorflow\n",
    "# !pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_io import write\n",
    "from model import model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFF\">\n",
    "an instance of the model (run the constructor) and attempt to reload a previously saved version from `sample_code_submission/`:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel = model()\n",
    "trained_model_name = model_dir + data_name\n",
    "# Uncomment the next line to re-load an already trained model\n",
    "#myModel = myModel.load(trained_model_name) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFF\">\n",
    "    Train the model (unless you reloaded a trained model) and make predictions. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN = data_train.drop('label', axis=1)\n",
    "Y_TRAIN = data_train[\"label\"]\n",
    "X_TEST = data_test.drop('label', axis=1)\n",
    "Y_TEST = data_test[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIT: dim(X)= [75, 4]\n",
      "FIT: dim(y)= [75, 1]\n",
      "PREDICT: dim(X)= [75, 4]\n",
      "PREDICT: dim(y)= [75, 1]\n",
      "PREDICT: dim(X)= [75, 4]\n",
      "PREDICT: dim(y)= [75, 1]\n"
     ]
    }
   ],
   "source": [
    "if not(myModel.is_trained):\n",
    "    myModel.fit(X_TRAIN, Y_TRAIN)                     \n",
    "\n",
    "Y_hat_train = myModel.predict(X_TRAIN) # Optional, not really needed to test on taining examples\n",
    "Y_hat_test = myModel.predict(X_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the trained model** (will be ready to reload next time around) and save the prediction results. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel.save(trained_model_name)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT:** if you save the trained model, it will be bundled with your sample code submission. Therefore your model will NOT be retrained on the challenge platform. Remove the pickle from the submission if you want the model to be retrained on the platform.\n",
    "\n",
    "**REQUIRED:** Trained model is required in the submission to codalab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_result_submission/iris_challenge_test.predict\n",
      "sample_result_submission/iris_challenge_train.predict\n"
     ]
    }
   ],
   "source": [
    "result_name = result_dir + data_name\n",
    "from data_io import write\n",
    "write(result_name + '_train.predict', Y_hat_train)\n",
    "write(result_name + '_test.predict', Y_hat_test)\n",
    "\n",
    "!ls $result_name*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring the results\n",
    "### Load the challenge metric\n",
    "\n",
    "**The metric chosen for your challenge** is identified in the \"metric.txt\" file found in the `scoring_program/` directory.\n",
    "<br> \n",
    "The function \"get_metric\" searches first for a metric having that name in my_metric.py, then in libscores.py, then in sklearn.metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using scoring metric: accuracy_score\n"
     ]
    }
   ],
   "source": [
    "from libscores import get_metric\n",
    "metric_name, scoring_function = get_metric()\n",
    "print('Using scoring metric:', metric_name)\n",
    "# Uncomment the next line to display the code of the scoring metric\n",
    "#??scoring_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score for the accuracy_score metric = 0.0000\n",
      "Ideal score for the accuracy_score metric = 1.0000\n",
      "Test score for the accuracy_score metric = 0.0000\n"
     ]
    }
   ],
   "source": [
    "print('Training score for the', metric_name, 'metric = %5.4f' % scoring_function(Y_TRAIN, Y_hat_train))\n",
    "print('Ideal score for the', metric_name, 'metric = %5.4f' % scoring_function(Y_TRAIN, Y_TRAIN))\n",
    "\n",
    "print('Test score for the', metric_name, 'metric = %5.4f' % scoring_function(Y_TEST, Y_hat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add here other scores and result visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation performance\n",
    "\n",
    "CV scores on sample_data doesn't have enough data, and so isn't meaningful.\n",
    "Run it with the full data to see meaningful values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIT: dim(X)= [50, 4]\n",
      "FIT: dim(y)= [50, 1]\n",
      "PREDICT: dim(X)= [25, 4]\n",
      "PREDICT: dim(y)= [25, 1]\n",
      "FIT: dim(X)= [50, 4]\n",
      "FIT: dim(y)= [50, 1]\n",
      "PREDICT: dim(X)= [25, 4]\n",
      "PREDICT: dim(y)= [25, 1]\n",
      "FIT: dim(X)= [50, 4]\n",
      "FIT: dim(y)= [50, 1]\n",
      "PREDICT: dim(X)= [25, 4]\n",
      "PREDICT: dim(y)= [25, 1]\n",
      "\n",
      "CV score (95 perc. CI): 0.00 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(myModel, X_TRAIN, Y_TRAIN, cv=3, scoring=make_scorer(scoring_function))\n",
    "print('\\nCV score (95 perc. CI): %0.2f (+/- %0.2f)' % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Step 3: Making a submission\n",
    "\n",
    "## Unit testing\n",
    "\n",
    "It is <b><span style=\"color:red\">important that you test your submission files before submitting them</span></b>. All you have to do to make a submission is modify the file <code>model.py</code> in the <code>sample_code_submission/</code> directory, then run this test to make sure everything works fine. This is the actual program that will be run on the server to test your submission. \n",
    "<br>\n",
    "Keep the sample code simple.<br>\n",
    "\n",
    "<code>python3</code> is required for this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using input_dir: /Users/ihsan/Desktop/create_a_challenge/starting_kit_tabular/sample_data\n",
      "Using output_dir: /Users/ihsan/Desktop/create_a_challenge/starting_kit_tabular/sample_result_submission\n",
      "Using program_dir: /Users/ihsan/Desktop/create_a_challenge/starting_kit_tabular/ingestion_program\n",
      "Using submission_dir: /Users/ihsan/Desktop/create_a_challenge/starting_kit_tabular/sample_code_submission\n",
      "Data name: iris_challenge\n",
      "\n",
      "========== Ingestion program version 6 ==========\n",
      "\n",
      "************************************************\n",
      "******** Processing dataset Iris_challenge ********\n",
      "************************************************\n",
      "========= Reading and converting data ==========\n",
      "###-------------------------------------###\n",
      "### Checking Data\n",
      "###-------------------------------------###\n",
      "\n",
      "\n",
      "###-------------------------------------###\n",
      "### Data Loaded!\n",
      "###-------------------------------------###\n",
      "\n",
      "\n",
      "[+] Cumulated time budget (all tasks so far)  500.00 sec\n",
      "[+] Time budget for this task 500.00 sec\n",
      "[+] Remaining time after reading data 500.00 sec\n",
      "======== Creating model ==========\n",
      "**********************************************************\n",
      "****** Attempting to reload model to avoid training ******\n",
      "**********************************************************\n",
      "Model reloaded from: /Users/ihsan/Desktop/create_a_challenge/starting_kit_tabular/sample_code_submission/iris_challenge_model.pickle\n",
      "[+] Model reloaded, no need to train!\n",
      "PREDICT: dim(X)= [75, 4]\n",
      "PREDICT: dim(y)= [75, 1]\n",
      "PREDICT: dim(X)= [75, 4]\n",
      "PREDICT: dim(y)= [75, 1]\n",
      "[+] Prediction success, time spent so far  0.00 sec\n",
      "======== Saving results to: /Users/ihsan/Desktop/create_a_challenge/starting_kit_tabular/sample_result_submission\n",
      "[+] Results saved, time spent so far  0.01 sec\n",
      "[+] End cycle, time left 499.99 sec\n",
      "[+] Done\n",
      "[+] Overall time spent  1.41 sec ::  Overall time budget 500.00 sec\n"
     ]
    }
   ],
   "source": [
    "# !source activate python3; \n",
    "!python3 $problem_dir/ingestion.py $data_dir $result_dir $problem_dir $model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test scoring program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###-------------------------------------###\n",
      "### Using metric :  accuracy_score\n",
      "###-------------------------------------###\n",
      "\n",
      "\n",
      "###-------------------------------------###\n",
      "### Total solutions :  150\n",
      "### Train solutions :  75\n",
      "### Test solutions :  75\n",
      "###-------------------------------------###\n",
      "\n",
      "\n",
      "###-------------------------------------###\n",
      "### Solutions files are ready!\n",
      "###-------------------------------------###\n",
      "\n",
      "\n",
      "======= Set 1 (Iris_challenge_train): accuracy_score(set1_score)=0.000000000000 =======\n",
      "======= Set 2 (Iris_challenge_test): accuracy_score(set2_score)=0.000000000000 =======\n"
     ]
    }
   ],
   "source": [
    "scoring_output_dir = 'scoring_output'\n",
    "# !source activate deeplearning; \n",
    "!python3 $score_dir/score.py $data_dir $result_dir $scoring_output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submit this file to codalab:\n",
      "../sample_code_submission_23-01-12-22-03.zip\n"
     ]
    }
   ],
   "source": [
    "import datetime \n",
    "from data_io import zipdir\n",
    "the_date = datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M\")\n",
    "sample_code_submission = '../sample_code_submission_' + the_date + '.zip'\n",
    "# sample_result_submission = '../sample_result_submission_' + the_date + '.zip'\n",
    "zipdir(sample_code_submission, model_dir)\n",
    "# zipdir(sample_result_submission, result_dir)\n",
    "print(\"Submit this file to codalab:\\n\" + sample_code_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fair-universe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c6da02e56cf1d01e59e2b6981d6c9e5f6a04b6b13974bc7babaa203713e631f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
